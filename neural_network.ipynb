{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport time\nprint(os.listdir(\"../input\"))\n\nimport matplotlib.pyplot as plt\n\n#Machine Learning Imports\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#Ml Core Algo Libraries\nimport lightgbm as lgb\nimport sklearn\n\n#Deep Learning Imports\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy, mse\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n#Competition Starter Kit\nfrom kaggle.competitions import twosigmanews\n\n#Gen imports\nfrom itertools import chain",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['marketdata_sample.csv', 'news_sample.csv']\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Create the env and get the data\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading the data... This could take a minute.\nDone!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38878f72bc98d6a788154db03b217f83b14e3028"
      },
      "cell_type": "code",
      "source": "#Define News Columns\nnews_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "645911907a1c114bd2e548cf5da127d7d2d1aeaf"
      },
      "cell_type": "code",
      "source": "#Define Join procedure for Market and News Data\ndef join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66792d862b17fa6ff6ebb788043d66bbb3d0d18e"
      },
      "cell_type": "code",
      "source": "#Splitting Data into X and Y and encoding the data\ndef get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y, le\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        #'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    #x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c8f80782670e83773c748ae7be03b2ab28fcf89"
      },
      "cell_type": "code",
      "source": "#Pick Data Sample - Try with 30k, 300k, 3M. Setting to 3M for final run.\nmerged_data, _ = get_x(market_train.tail(3000000), news_train.tail(6000000))\n#print (merged_data.shape)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  app.launch_new_instance()\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  after removing the cwd from sys.path.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9390593c15c9f4288ba8d69596c8183e8b90a32"
      },
      "cell_type": "code",
      "source": "#Defining the categorical and numerical columns. Categorical Columns will be feeded separately into the NN through cat layers after encoding. \n#Numerical columns will be feeded directly.\n\ncat_cols = ['assetCode', 'assetName']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10', 'urgency_min', 'relevance_mean', 'sentimentNegative_mean', 'sentimentNeutral_mean',\n    'sentimentPositive_mean', 'noveltyCount24H_mean', 'volumeCounts24H_mean']\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07315fb151d66fa3d0044dceea55366a00564668"
      },
      "cell_type": "code",
      "source": "market_train = merged_data\n\ntrain, validate, test = np.split(merged_data.sample(frac=1), [int(.6*len(merged_data)), int(.8*len(merged_data))])\n\n#Debug code\n#print (train)\n#print(validate)\n#print (test)\n\ntrain_indices = train.index.values\nval_indices = validate.index.values\ntest_indices = test.index.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "567208286b8152229b707593e48c67254ffc24b2"
      },
      "cell_type": "code",
      "source": "#Categorical Columns Processing: Encoding of the categorical variables\n\ndef encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\n#Handling the unknown assets: Using +1 for unknown\nembed_sizes = [len(encoder) + 1 for encoder in encoders]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a55a27b6a5e8f04d92291cd8185c9f8fcd3bb89"
      },
      "cell_type": "code",
      "source": "#Numerical Columns Processing: Scaling Numerical Columns\n\nmarket_train[num_cols] = market_train[num_cols].fillna(0)\n\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "facfbb44ca6c3a6e818c2b11cef42aa5fbcfed0b"
      },
      "cell_type": "code",
      "source": "#Defining the NN Architecture: Using the MSE as a Loss. tanh output activation since confidence is between -1.0 and 1.0\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[len(cat_cols)], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\nnumerical_inputs = Input(shape=(len(num_cols),), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='tanh')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=mse)\n\n#model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49732798c8156671e8dfc4316b0280e04c3a9e11"
      },
      "cell_type": "code",
      "source": "def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)\nX_test,y_test,r_test,u_test,d_test = get_input(market_train, test_indices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fbd233b482d313067bb7704e059b5cb5f011bf5"
      },
      "cell_type": "code",
      "source": "check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=5,verbose=True)\n\n#Regression type fitting\nmodel.fit(X_train,r_train,\n          validation_data=(X_valid,r_valid),\n          epochs=10,\n          verbose=True,\n          callbacks=[early_stop,check_point])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2c5dd85e08bba1fbc5a99deb490c785ad6ca63e"
      },
      "cell_type": "code",
      "source": "#plot confidence interval\ny_pred = model.predict(X_test)[:,0]\nplt.hist(r_test, bins=np.linspace(-0.5,0.5,150), alpha=0.3)\nplt.hist(y_pred, bins=np.linspace(-0.5,0.5,150), alpha=0.3, color='darkorange')\nplt.legend(['Ground truth', 'Predicted'])\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f53f13c06f05b90bb74afb307a591b03cc4f79eb"
      },
      "cell_type": "code",
      "source": "#Print Accuracies\nprint(accuracy_score(model.predict(X_train)[:,0]>0,y_train))\nprint(accuracy_score(model.predict(X_valid)[:,0]>0,y_valid))\nprint(accuracy_score(model.predict(X_test)[:,0]>0,y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "654d8c67bdd7bf2b9d46d5bb02734b0fb3aa1ac0"
      },
      "cell_type": "code",
      "source": "#Print MSEs\nprint(mean_squared_error(model.predict(X_train)[:,0],r_train))\nprint(mean_squared_error(model.predict(X_valid)[:,0],r_valid))\nprint(mean_squared_error(model.predict(X_test)[:,0],r_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "350cda749a634a2c1dd0d78b7e2a922a6d4152e8"
      },
      "cell_type": "code",
      "source": "#Competition Submission\ndays = env.get_prediction_days()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8fb84e079225a86b03cfec5d8996b261289a794"
      },
      "cell_type": "code",
      "source": "#Make predictions and save file\n\"\"\"\nn_days = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days += 1\n    \n    market_obs_df_updated, _ = get_x(market_obs_df, news_obs_df)\n    \n    for i, cat in enumerate(cat_cols):\n        market_obs_df_updated[cat] = market_obs_df_updated[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    \n    market_obs_df_updated[num_cols] = market_obs_df_updated[num_cols].fillna(0)\n    market_obs_df_updated[num_cols] = scaler.transform(market_obs_df_updated[num_cols])\n    \n    X_num_test = market_obs_df_updated[num_cols].values\n    X_test = {'num':X_num_test}\n    for cat in cat_cols:\n        X_test[cat] = market_obs_df_updated[cat_cols].values    \n    \n    market_prediction = model.predict(X_test)[:,0]\n    \n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    \n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    predictions_template_df.confidence_value = market_prediction\n    env.predict(predictions_template_df)\n    \nenv.write_submission_file()\n\"\"\"",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}